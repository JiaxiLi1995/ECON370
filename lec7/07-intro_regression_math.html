<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Drew Van Kuiken" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data Science for Economists
]
.subtitle[
## Lecture 7: Introduction to Regression - Math
]
.author[
### Drew Van Kuiken
]
.date[
### University of North Carolina | <a href="https://github.com/drewvankuiken/ECON370">ECON 370</a>
]

---

name: toc


# Table of contents

1. [Introduction](#intro)

2. [Regression: An Intuitive Approach](#intuit)

3. [Regression: Some Math](#math) 

---
# An Easy Question

What's the line of best fit when our data looks like this? 


``` r
d = data.frame(x=1:2, y=3:4)
```

--

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

$$ Y = mX + B  \Rightarrow Y = \beta_0 + \beta_1X$$
---
class:inverse
$$\beta_1 = \frac{\text{Rise}}{\text{Run}} = \frac{4-3}{2-1} = 1 $$

$$\beta_0 = Y - \beta_1 X = Y - X = 3 - 1 = 2 $$

&lt;/br&gt;

We need a little more notation now: 
- Individual observation `\(i\)` is referred to as `\((x_i,y_i)\)`
- Predicted y is given by `\(\hat{y}\)`
- Average y is given by `\(\bar{y}\)` (average x is given by `\(\bar{x}\)`)

---
# How about three points? 

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

No longer a perfect fit. Is the following correct? 

$$y_i = \beta_0 + \beta_1 x_i $$

---
# Error

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

The dashed lines are ***errors***. Our *line of best fit* is still given by: 

$$ \hat{y} = \beta_0 + \beta_1 x_i $$

But for any given point, we need to add in those errors: if an observation `\(i\)` is given by `\((x_i,y_i)\)`, then for values of `\(\beta_0, \beta_1\)` 

`$$y_i = \beta_0 + \beta_1 x_i +\varepsilon_i$$` 

---
# Calculating Errors

`$$\varepsilon_i(\beta) = y_i - \beta_0 - \beta_1 x_i$$`
Note that this formula implicitly compares *predicted* y, which we call `\(\hat{y}\)` to *observed* y, which is `\(y_i\)`

In this case: 
$$ \varepsilon_1 = 3 - 1 - 1.5(1) = 0.5 $$
$$ \varepsilon_2 = 3 - 1 - 1.5(2) = -1 $$
$$ \varepsilon_3 = 6 - 1 - 1.5(3) = 0.5 $$

The sum of our squared residuals: 

$$ \sum_{i=1}^3 \varepsilon_i^2 = -0.5^2 + 1^2+ -0.5^2 = 1.5 $$

--

We said a regression **minimizes** errors

---
# Minimizing Error

How can we minimize error? 

Choose `\(\beta_0, \beta_1\)` to "minimize" the total (or sum of) squared error,

`$$\min_{\beta_0, \beta_1}\sum_{i=1}^{N} \varepsilon_i(\beta_0, \beta_1)^2$$`

--

Why squared? 
--

1. So positive and negative errors don't cancel out
2. So larger errors contain more weight

---
# How to Find `\(\beta_0, \beta_1\)`: R Code

Running regressions in `R` is simple. Let's start by downloading a real dataset: 

``` r
url = "https://www.statlearning.com/s/Advertising.csv"
advert_data = read_csv(url)
tv_reg &lt;- lm(sales ~ TV, advert_data)
summary(tv_reg)
```

```
## 
## Call:
## lm(formula = sales ~ TV, data = advert_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3860 -1.9545 -0.1913  2.0671  7.2124 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 7.032594   0.457843   15.36   &lt;2e-16 ***
## TV          0.047537   0.002691   17.67   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.259 on 198 degrees of freedom
## Multiple R-squared:  0.6119,	Adjusted R-squared:  0.6099 
## F-statistic: 312.1 on 1 and 198 DF,  p-value: &lt; 2.2e-16
```

---
# How to Find `\(\beta_0, \beta_1\)`: R Code

General notes on coding regressions. Okay if this doesn't make sense now, should be useful later:
- Anything left of `~` is our dependent variable. To the right is our formula
- `summary()` will return useful information on your regression
- `:` codes interactions. Think of this as: what is the effect of `\(x_1\)` and `\(x_2\)` together
  - Conditional on `\(x_1\)`, what is the effect of `\(x_2\)`? 
- as.factor(x) will tell R that x is a dummy variable
- use `I(x^2)` to fit higher order polynomials

---
# How to Find `\(\beta_0, \beta_1\)`: Algebra

How do we find the minimum of a function? Take the derivative a set it equal to 0! But we'll skip this today. 

Instead, take it from me that we end up with the following formulas: 

$$ \beta_0 = \bar{y} - \hat{\beta}\bar{x} $$

`$$\beta_1 = \frac{\sum_{i=1}^N(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^N (x_i - \bar{x})^2}$$`

Take 5 minutes and code up our regression coefficients using just algebra for the Sales ~ TV regression

---
# How to Find `\(\beta_0, \beta_1\)`: Plug and Chug

We can also write an algorithm to iteratively minimize `\(f(\beta_0, \beta_1) = \sum_{i=1}^N \varepsilon (\beta_0,\beta_1)^2\)`. Any ideas?

--


``` r
# grid search for coefficients
beta = expand.grid(b0 = seq(5,10,0.25), b1 = seq(-0.1,0.1,0.01))
tss = sapply(1:nrow(beta), function(i) {
  sum((advert_data$sales - beta$b0[i] - beta$b1[i] * advert_data$TV)^2)
})
beta[tss==min(tss),]
```

```
##       b0   b1
## 323 6.75 0.05
```

---
# Plot of Objective Function `\(f(\beta_0, \beta_1)\)`

We can visualize the results. Let's start with `\(\beta_0 = 5\)`:



``` r
g1
```

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

---
# Plot of Objective Function `\(f(\beta_0, \beta_1)\)`

We can visualize the results. Let's add a couple more: 




``` r
g2
```

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-10-1.svg" style="display: block; margin: auto;" /&gt;

Notice the green line is lower than the blue and red lines


---
# Increasing Our Grid Search




``` r
# more precise 
beta2 = expand.grid(b0 = seq(6,8,0.01), b1 = seq(-0.1,0.1,0.0005))
tss2 = sapply(1:nrow(beta2), function(i) {
  sum((advert_data$sales - beta2$b0[i] - beta2$b1[i] * advert_data$TV)^2)
})
beta2[tss2==min(tss2),]
```

```
##         b0     b1
## 59400 7.04 0.0475
```

---
# This Also Makes Our Graph Look Better



``` r
g3
```

&lt;img src="07-intro_regression_math_files/figure-html/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;

---
# This Is Optimization!


``` r
# optimization
f = function(theta) sum((advert_data$sales-theta[1]-theta[2]*advert_data$TV)^2) # form objective function

optim(c(mean(advert_data$sales),0),f,method="BFGS")$par # solve numerically with optim
```

```
## [1] 7.03259355 0.04753664
```

``` r
coef(lm(sales~TV,data=advert_data))     # solve with OLS
```

```
## (Intercept)          TV 
##  7.03259355  0.04753664
```

---
# Assessing Model Fit

Coefficients have standard errors
- Calculated as `\(\sqrt{\frac{\sigma^2_\varepsilon}{\sum_1^n (x_i - \bar{x})^2}}\)`
- Unexplained variation in y as a share of variation in x
- More variation in x `\(\Rightarrow\)` more leverage to estimate `\(\beta_1\)`
--

- Can use standard errors to construct a confidence interval for `\(\beta_1\)`: the range of values such that, with 95% probability, the range will contain the true unknown value of `\(\beta_1\)`
- 95% CI: `\([\beta_1 - 2 SE(\beta_1), \beta_1 + 2 SE(\beta_1)]\)`
- P-values assess the likelihood that your coefficient is different than 0 due to random chance
- Small p-values `\(\Rightarrow\)` can infer there is a relationship between independent variable and dependent variable

---
# Assessing Model Fit

Models have R-squared values
- Calculated as `\(1 - \frac{\sum_1^n(y_i - \hat{y})^2}{\sum_1^n(y_i - \bar{y})^2}\)`
- Variation in y unexplained by predictors divided by total variance in y
- I.e., how much of the total variation in y does your model explain? 
- In OLS, R-Squared = Corr(X,Y) squared!
--

- Runs from 0 to 1, 0 is explains nothing, 1 is a perfect fit
- What's a good r-squared? Depends
  - Cryptocurrency transaction data? 0.00008 was great
  - Testing lab data from physics experiment? Should be ~1

---
# Multivariate Regression

In our gapminder regression, we added an indicator variable for "defender in a war." This was our first example of multivariable regression. 

This extension is fairly simple. Each explanatory variable gets its own slope. Our regression model becomes: 

`$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} + \varepsilon$$`

Interpretation: what's the average effect of a one-unit increase in `\(x_1\)` on `\(y\)` holding all other `\(x\)` variables fixed? 

Estimating the coefficients (the `\(\beta\)`s) is a little trickier. Need matrix algebra to represent the closed-form solution. We are still minimizing squared errors though! Now it's just in `\(p\)` dimensions. 

---
# Dummy Variables 


``` r
library(gapminder)
gp_subset &lt;- gapminder[gapminder$continent=="Asia"&amp;gapminder$year==2007,]
gp_subset$long_and_brutal_war = gp_subset[,"country"] == "Afghanistan"
mv_reg &lt;- lm(lifeExp ~ gdpPercap + long_and_brutal_war, data = gp_subset)
summary(mv_reg)
```

```
## 
## Call:
## lm(formula = lifeExp ~ gdpPercap + long_and_brutal_war, data = gp_subset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.1358 -3.2854  0.7948  3.4495  6.2692 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              6.714e+01  1.012e+00  66.340  &lt; 2e-16 ***
## gdpPercap                3.454e-04  5.328e-05   6.483 3.64e-07 ***
## long_and_brutal_warTRUE -2.365e+01  4.332e+00  -5.458 6.40e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.221 on 30 degrees of freedom
## Multiple R-squared:  0.7367,	Adjusted R-squared:  0.7191 
## F-statistic: 41.96 on 2 and 30 DF,  p-value: 2.03e-09
```

What's the interpretation of `long_and_brutal_war`? What does it tell us? 
---
# Dummy Variables


``` r
mv_reg2 &lt;- lm(lifeExp ~ gdpPercap + long_and_brutal_war - 1, data = gp_subset)
summary(mv_reg2)
```

```
## 
## Call:
## lm(formula = lifeExp ~ gdpPercap + long_and_brutal_war - 1, data = gp_subset)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.1358 -3.2854  0.7948  3.4495  6.2692 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## gdpPercap                3.454e-04  5.328e-05   6.483 3.64e-07 ***
## long_and_brutal_warFALSE 6.714e+01  1.012e+00  66.340  &lt; 2e-16 ***
## long_and_brutal_warTRUE  4.349e+01  4.221e+00  10.304 2.27e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.221 on 30 degrees of freedom
## Multiple R-squared:  0.9968,	Adjusted R-squared:  0.9965 
## F-statistic:  3117 on 3 and 30 DF,  p-value: &lt; 2.2e-16
```

``` r
mean(gp_subset$lifeExp[gp_subset$long_and_brutal_war!=1])
```

```
## [1] 71.56912
```

``` r
mean(gp_subset$lifeExp[gp_subset$long_and_brutal_war==1])
```

```
## [1] 43.828
```

---
# Last note

In class, we discussed what it would mean if having a lower GDP per capita meant that you were more likely be the defender in a war. For this class, that is okay. As long as we can hold "being at war" and "gdp per capita" fixed, we've dealt with omitted variable bias. 

As you learn more econometrics, you'll learn more about exogeneity, endogeneity, and causal inference. 

---
# We Covered OLS

The reason we had an "algebra" version of these slides is because OLS is really nice. It has a closed form solution!
- If you've taken 400 or 470, you may have seen the formula for OLS

--

However, not all optimization problems will have a closed form solution.
- Will need numerical methods and optimization techniques to find solutions.
- We'll cover this a little later in the course. 

---

class: inverse, center, middle

# Next lecture(s): Simulation

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
