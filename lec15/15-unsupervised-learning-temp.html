<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Drew Van Kuiken" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data Science for Economists
]
.subtitle[
## Lecture 15: Unsupervised Learning
]
.author[
### Drew Van Kuiken
]
.date[
### University of North Carolina | <a href="https://github.com/drewvankuiken/ECON370">ECON 370</a>
]

---







# Table of contents

1. [Introduction](#intro)

2. [Unsupervised Learning](#unsup)

3. [Clustering](#clustering)

 - [K-Means](#kmeans)
 - [EM-Algorithn](#EM)


---
class: inverse, center, middle
name: intro

# Introduction

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Motivation

We've already touched briefly on what unsupervised learning is.

These methods are the methods that y'all have likely not seen.

The idea of these methods is for when there's something unobserved you would like to either reduce or group together.


---
class: inverse, center, middle
name: unsup

# Unsupervised Learning

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Unsupervised Learning

Unsupervised learning is a bit more challenging and less understood than supervised learning.

&gt; We are not interested in prediction, because we do not have an associated response variable `\(Y\)`. Rather, the goal is to discover interesting things about the measurements on `\(X_1, X_2, ..., X_d\)`.

There is no simple goal and is often more subjective. 

Is largely part of exploratory analysis.

There are two common unsupervised learning algorithms: 
1. Principal Components Analysis (PCA)
2. Clustering

We are going to skip PCA.

---
class: inverse, center, middle
name: clustering

# Clustering

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Clustering

Clustering refers to a broad set of techniques to find subgroups or "clusters" in data.

With clustering, we often have a reason to believe that there is "heterogeneity" among groups, but don't really know how to label this heterogeneity. 

Clustering can be used to find these groups.

Example: Market segmentation.

--

There are two types of clustering:
1. Hard Clustering: assign groups to observations with certainty (probability 1).
2. Soft Clustering: assign probabilities of belonging to each group to all observations. These probabilities can be between 0 and 1.

---
class: inverse, center, middle
name: kmeans

# K-Means Clustering

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# K-Means

`\(K\)`-means clustering is a simple and elegant approach for partitioning a data set into `\(K\)` distinct, non-overlapping clusters.

--

`\(K\)` must be specified and then each observation is assigned to one of the `\(K\)` groups.

&lt;img src="pics/kmeans.png" width="80%" style="display: block; margin: auto;" /&gt;

---
# K-Means

Let `\(C_1,...,C_K\)` denote sets containing the indicies of the observations in each cluster.

1. `\(C_1\cup C_2 ... \cup C_K = \{1,...,n\}\)`. In words, each observation belongs to at least one of the `\(K\)` clusters.

2. `\(C_k \cap C_{k'} = \emptyset\)` for all `\(k\neq k'\)`. In words, the clusters are non- overlapping: no observation belongs to more than one cluster.

We want to choose `\(C_1,...,C_K\)` in such a way that they solve: `$$\min_{C_1,...,C_K} \sum_{k=1}^{K} W(C_k)$$` where `\(W(C_k)\)` is some measure of "with-in cluster" variation of cluster `\(k\)`.
- Minimizes the "within group variance."

The most common choice of `\(W(C_k)\)` is `$$W\left(C_{k}\right)=\frac{1}{\left|C_{k}\right|} \sum_{i, i^{\prime} \in C_{k}} \sum_{j=1}^{d}\left(x_{i j}-x_{i^{\prime} j}\right)^{2}$$`

---
# K-Means

Putting this all together, we want to choose `\(C_1,...,C_K\)` s.t. they solve the following optimization problem `$$\min_{C_1,...,C_K} \sum_{k=1}^{K} \frac{1}{\left|C_{k}\right|} \sum_{i, i^{\prime} \in C_{k}} \sum_{j=1}^{d}\left(x_{i j}-x_{i^{\prime} j}\right)^{2}$$`

--

Now, how do we solve?

--

This is actually a hard problem to solve. Fortunately, there is a way to find a local minima.
 - Not guaranteed to achieve the global minimum.
 - Remember everything we talked about in the optimization lecture.
  - e.g. Could solve multiple times and pick the solution with the best fit criterion.

---
# K-Means Algorithm

The `\(K\)`-means minimization problem can be solved using the following algorithm.

1. Randomly assign a number, from 1 to `\(K\)`, to each of the observations. These serve as initial cluster assignments for the observations.
2. Iterate until the cluster assignments stop changing:
 - For each of the `\(K\)` clusters, compute the cluster *centroid*. The `\(k^\text{th}\)` cluster *centroid* is the vector of the `\(d\)` feature means for the observations in the `\(k^\text{th}\)` cluster.
 - Assign each observation to the cluster whose *centroid* is closest (where closest is defined using Euclidean distance).

Let `\(\boldsymbol{\mu}^k=(\mu^k_1,...,\mu^k_d)\)` be the centroid of cluster `\(k\)` and `\(\boldsymbol{x}_i=(x_{i1},...,x_{id})\)`.

Euclidean Distance/Norm:

`$$||\boldsymbol{x}_i-\boldsymbol{\mu}^k||_2=\Big(\sum_{j=1}^{d}(x_{ij}-\mu^k_j)^2\Big)^{\frac{1}{2}}=\sqrt{\sum_{j=1}^{d}(x_{ij}-\mu^k_j)^2}$$`
---
# K-Means Algorithm

&lt;img src="pics/kmeans-algo.png" width="72%" style="display: block; margin: auto;" /&gt;

---
# K-Means Algorithm

&lt;img src="pics/kmeans-local.png" width="72%" style="display: block; margin: auto;" /&gt;

---
#K-Means Example

Generate some data and show you how the `\(K\)`-means algorithm is programmed.
- Don't worry: There is a `\(K\)`-means function that we will use later.


``` r
#requires the following packages: mvtnorm, data.table, ggpubr
set.seed(123)
N = 100000; K = 2; P = 2; NK=N*K # set parameters to generate data
data_MC1 = rmvnorm(N,c(17,17),matrix(c(10,0,0,10),ncol=2)) # draw data group 1
data_MC2 = rmvnorm(N,c(10,10),matrix(c(10,9,9,10),ncol=2)) # draw data group 2
data_MC  = as.data.table(rbind(data_MC1,data_MC2)) # rbind the two datasets
data_MC[,group:=ifelse(.I/.N&gt;0.5,2,1)]    # assign groups
setnames(data_MC,c("V1","V2"),c("x","y")) # rename variables
data_MC[c(1:3,(.N-2):.N)]                 # show first and last 3 rows
```

```
##            x         y group
##        &lt;num&gt;     &lt;num&gt; &lt;num&gt;
## 1: 15.227620 16.272115     1
## 2: 21.929068 17.222967     1
## 3: 17.408844 22.423512     1
## 4:  9.560285  9.761522     2
## 5: 10.496481  8.539214     2
## 6:  6.788124  7.700238     2
```

---
#K-Means Example: Scatter Plots

&lt;img src="15-unsupervised-learning-temp_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

Two clear groups but there's lots of overlap between the two.
- Without the labels, cannot know for sure which group where there is overlap.

---
#K-Means Example: Distribution Densities

To better understand the overlap, look at the joint density (distribution).
- 3D plot looked at from above

&lt;img src="15-unsupervised-learning-temp_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---
# K-Means Algorithm: First Step


``` r
data_MC[,group_assign:=sample(1:K,K*N,replace=T)] # randomly assign initial groups
## --- Calculate centroids
# Centroids are just the average of every variable by assigned group 
centroids = data_MC[,lapply(.SD, mean),by=group_assign,.SDcols=c("x","y")] 
setorder(centroids,group_assign); centroids # sort by assigned group and print
```

```
##    group_assign        x        y
##           &lt;int&gt;    &lt;num&gt;    &lt;num&gt;
## 1:            1 13.50611 13.51900
## 2:            2 13.49868 13.49565
```

``` r
dat_mat = as.matrix(data_MC[,.(x,y)]) # store data as a matrix
Dist    = array(0,dim=c(NK,K))        # initialize distance matrix
## --- Define norm function for calculating distances
# p == 2 is the Euclidean distance/Pythogreon theorem. p == Inf is sup-norm
norm = function(x,p=2) if(p==Inf) max(abs(x)) else sum(abs(x)^p)^(1/p)

## --- For each group and observation, calculate distance to each centroid 
for(k in 1:K){
  cent_mat = centroids[rep(k,NK),.(x,y)]    # store centroids for group k
  cent_mat = as.matrix(cent_mat)            # guarantee cent_mat is a matrix
  Dist[,k] = apply(dat_mat-cent_mat,1,norm) # calculate Euclidean Distance
}
new_groups = apply(Dist,1,which.min) # new group is whichever has the smallest distance
```

---
# K-Means Algorithm: While Loop


``` r
## --- Continue the same process until no observations change groups
while(sum(data_MC$group_assign != new_groups)&gt;0){
  data_MC[,group_assign:=new_groups] # assign new groups
  ## --- Calculate centroids
  centroids = data_MC[,lapply(.SD, mean),by=group_assign,.SDcols=c("x","y")] 
  centroids = centroids[order(group_assign)] # sort centroids by group

  for(k in 1:K){
    cent_mat = centroids[rep(k,NK),.(x,y)]    # store centroids for k
    cent_mat = as.matrix(cent_mat)            # make cent_mat a matrix
    Dist[,k] = apply(dat_mat-cent_mat,1,norm) # calculate Euclidean distance
  }
  new_groups = apply(Dist,1,which.min) # new group is whichever has smallest distance
}

data_MC[,group_assign:=new_groups] # assign final groups
centroids
```

```
##    group_assign         x         y
##           &lt;int&gt;     &lt;num&gt;     &lt;num&gt;
## 1:            1 16.840610 16.848321
## 2:            2  9.237344  9.238737
```

---
# K-Means Example: kmeans Function

`R` has a built-in `\(K\)`-means function.

``` r
kmeans_out = kmeans(data_MC[,.(x,y)],centers=2,nstart=10,iter.max=100)
kmeans_out$centers
```

```
##           x        y
## 1 16.840574 16.84809
## 2  9.237044  9.23869
```

Very similar results.
- `nstart` is how many times we solve the problem

--


``` r
kmeans_out$withinss # within cluster sum-of-squares
```

```
## [1] 2025686 1192543
```

``` r
kmeans_out$tot.withinss # total within sum-of-squares
```

```
## [1] 3218229
```

`tot.withinss` is what the algorithm is trying to minimize.

---
# K-Means Example 

&lt;img src="15-unsupervised-learning-temp_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

K-Means does not do well when there is lots of overlap of the groups.

---
class: inverse, center, middle
name: EM

# Soft Clustering: The EM Algorithm

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Soft Clustering

Notice that `\(K\)`-means performs poorly when there's lots of overlap in the data.

Idea: What if we model the clustering and assign probabilities to the group?

Downsides: Must specify a model for the data to be generated from.

Upsides: Can assign probabilities.

--

The rest of the lecture, we will be using the following example to motivate the method:

### Differences in height (and weight) by sex

---
# Height Differences by Sex: NHIS Data

From the CDC's website, you can download data from the National Health Interview Survey. 

[We will be using the adult 2019 data.](https://www.cdc.gov/nchs/nhis/2019nhis.htm)

This is real data gathered from the US population regarding health. 


``` r
data_path = "/Users/drewvankuiken/Dropbox/ECON370/local/lec15/"
NHIS_data = fread(paste0(data_path,"adult19.csv"))
NHIS_data = NHIS_data[SEX_A&lt;7]    #remove responses not male or female
setnames(NHIS_data,c("HEIGHTTC_A","WEIGHTLBTC_A"),c("height","weight"))
NHIS_data = NHIS_data[height&lt;96]  #remove non-numerical responses
NHIS_data = NHIS_data[weight&lt;900] #remove non-numerical responses
NHIS_data[,sex:="Male"]; NHIS_data[SEX_A==2,sex:="Female"] # create sex var
NHIS_plot_data = copy(NHIS_data); NHIS_plot_data[,sex:="All"]
NHIS_plot_data = rbind(NHIS_plot_data[,.(sex,height,weight)],
                       NHIS_data[,.(sex,height,weight)])
hght_moms = NHIS_plot_data[,list("mu"=mean(height),"var"=var(height)),by=sex]
hght_moms[order(sex,decreasing = T),]
```

```
##       sex       mu       var
##    &lt;char&gt;    &lt;num&gt;     &lt;num&gt;
## 1:   Male 69.79632  7.953308
## 2: Female 64.11915  6.332597
## 3:    All 66.77460 15.114710
```


---
# Distribution of Height by Sex



&lt;img src="15-unsupervised-learning-temp_files/figure-html/CreateHeightPlotBySex-1.png" style="display: block; margin: auto;" /&gt;

Each empirical distribution (i.e. the distribution of the data) by sex looks normal! 
- The "total" distribution is a *mixture* of males and females.

---
# Model for Sex Differences in Height

It seems reasonable to assume that height is normally distributed conditional on sex where males and females have different distributions.
- If sex is labeled in the data, then can simply group on sex.

--

What if we don't observe someone's sex? How can we model this?
- To be clear, this is a thought experiment. There is a variable for sex in the data. 
- This is to show you what to do if we didn't.

--

Simple Statistical Model for Height
- Person `\(i\)` is assigned `\(M\)` with probability `\(\alpha\)` and `\(F\)` with probability `\(1-\alpha\)`
- `\(s_i \in\{M,F\}\)` and `\(s_i\)` is person `\(i\)`'s sex.
- Assume height `\(h_i\)` is drawn from `\(N(\mu_{s_i},\sigma^2_{s_i})\)` 
 - Each person's height is drawn from a distribution that depends upon their sex
- This is called a Normal (or Gaussian) Mixture Model
 - The entire population's height distribution is a mixture of two normals where each sex has its own distribution.

--

Given a sample of `\((h_i)_{i=1}^{N}\)`, how can we estimate `\(\mu_M, \mu_F, \sigma^2_M,\sigma^2_F,\)` and `\(\alpha\)`?

---
# Expectation-Maximization (EM)
An algorithm called the EM algorithm can be used to estimate the parameters.
- Today we discuss it in the context of a Gaussian Mixture Model, but it can be used for any model where one variable is a mixture of different groups and the group labels are not observed.

--

There are two steps:
1. **The Expectation Step**: calculate probability each observation belongs a group.
2. **The Maximization Step**: re-estimate the parameters given the new probabilities.

This requires something called Bayes' Rule. 

---
# Bayes' Rule and EM

Denote `\(P(M|h_i)=P(\text{sex}_i = M | \text{height}=h_i)\)` i.e. the prob `\(i\)` is male *given* `\(i\)`'s height `\(h_i\)`.

Bayes's rule states:

`$$P(M|h_i) = \frac{P(h_i|M)P(M)}{P(h_i|M)P(M)+P(h_i|F)P(F))}$$`
--

- `\(P(M) = \alpha\)` and `\(P(F)=1-\alpha\)` are the baseline probs of being `\(M/F\)`. 
- `\(P(h_i|M)\)` is the "probability" of height `\(h_i\)` if we **knew** the individual was male. Since our model assumes `\(h_i \sim N(\mu_M,\sigma^2_M)\)` if male, we know `\(P(h_i|M) = \phi(h_i;\mu_M,\sigma^2_M)\)`
 - That is `\(\phi\)` is the pdf of the normal distribution; the `dnorm` function in `R`.

`$$P(M|h_i) = \frac{\phi(h_i;\mu_M,\sigma^2_M)\alpha}{\phi(h_i;\mu_M,\sigma^2_M)\alpha+\phi(h_i;\mu_F,\sigma^2_F)(1-\alpha)}$$`
--

If we knew `\((\mu_M,\mu_F,\sigma^2_M,\sigma^2_F,\alpha)\)`, we could compute `\(P(M|h_i)\)`.

However, we need `\(P(M|h_i)\)` to estimate `\((\mu_M,\mu_F,\sigma^2_M,\sigma^2_F,\alpha)\)`.

The EM algorithm allows us to do both by alternating between the E and M steps.

---
# EM Algorithm

Denote `\(\theta_n = (\mu_{M,n},\mu_{F,n},\sigma^2_{M,n},\sigma^2_{F,n},\alpha_n)\)` as the parameter estimates after `\(n\)` iterations.

--

Step 0: Initialize `\(\theta_0\)` somehow. 
- The easiest way to do this is to run `\(K\)`-means.
- Use the resulting groups from `\(K\)`-means to calculate group means, variances, and proportions.
- Assume males are the group with the larger mean height.
- This gives us initial guesses of `\(\theta_0=(\mu_{M,0},\mu_{F,0},\sigma^2_{M,0},\sigma^2_{F,0}, \alpha_0)\)`.

--

Step 1: 
 - **E-Step**: Calculate `\(P(M|h_i;\theta_0) = \pi^M_1(h_i)\)` using Bayes' Rule. 
  - Note `\(\pi^F_1(h_i) = 1-\pi^M_1(h_i)\)`
 - **M-Step**: Calculate `\(\theta_1\)` given `\(\pi^M_1(h_i)\)` and the data. Formulas on next slide.
  - These are basically just weighted versions of the sample mean and sample variance where the weights are `\(\pi^M_1(h_i)\)` and `\(\pi^F_1(h_i)\)` for males and females respectively.
 
--
 
Step n: Continue alternating between the E-Step and M-Step until `\(||\theta_n - \theta_{n-1}||_p &lt; \varepsilon\)`

---
# Normal Mixture Formulas

`$$N_M = \sum_{i=1}^{N} \pi^M_{n}(h_i)$$`
`$$N_F = N - N_M$$`

`$$\mu_{M,n} = \frac{1}{N_M}\sum_{i=1}^{N}\pi^M_{n}(h_i)h_i$$` `$$\mu_{F,n} = \frac{1}{N_F}\sum_{i=1}^{N}\pi^F_{n}(h_i)h_i$$`
`$$\sigma^2_{M,n} = \frac{1}{N_M}\sum_{i=1}^{N}\pi^M_{n}(h_i)(h_i-\mu_{M,n})^2$$`
`$$\sigma^2_{F,n} = \frac{1}{N_F}\sum_{i=1}^{N}\pi^F_{n}(h_i)(h_i-\mu_{F,n})^2$$`
`$$\alpha_n = \frac{N_M}{N}$$`
---
# Functions for Parameter Estimation

The formulas on the last slides are essentially weighted means and variances 
- The weights are the probability of belonging to the group.


``` r
## --- Function for weighted mean
mean_wgt = function(x,ws) sum(ws*x)/sum(ws)
## --- Function for weighted variance
var_wgt = function(x,ws) sum(ws*(x-mean_wgt(x,ws))^2)/sum(ws)
## --- Function for weight sample proportion
prop_wgt = function(ws) sum(ws)/length(ws)
```

---
# EM Algorithm: Set-Up

Use K-means with `\(K=2\)` to classify observations into two initial groups.
- Use these groups to get initial calculations of `\((\mu_M,\mu_F,\sigma^2_M,\sigma^2_F,\alpha)\)`.


``` r
## ---- Initial clusters using k-means 
his   = NHIS_data[,height]           # store heights
h_kms = kmeans(NHIS_data[,height],2) # run k-means with k=2

## ---- Create table of parameters based on the k-means clusters
M_id = which.max(h_kms$centers)                    # get male id based on max height
tdat = data.table(height=his,id=h_kms$cluster)     # create table of heights and ids 
tdat[,sex:=ifelse(id==M_id,"Male","Female")]       # add sex labels to ids
tdat = tdat[,list(mu=mean(height),sig2=var(height),# calc mu's, sig2's, and probs
                  prob=.N/nrow(tdat)),by=sex]      # by sex
tdat = tdat[order(sex,decreasing=T)]               # make sure Male is first row
theta0 = c(tdat[,mu],tdat[,sig2],tdat[1,prob])    # form theta0
names(theta0)=c("muM","muF","sig2M","sig2F","alpha") # give names to elements of theta0
theta0
```

```
##        muM        muF      sig2M      sig2F      alpha 
## 70.6040213 64.0273771  4.4340449  4.7091539  0.4177237
```

``` r
theta1 = theta0 # initialize theta1
```

---
# EM Algorithm: Initial Step


``` r
#E-Step
mu0s  = theta0[c("muM","muF")]                # store mus
sig0s = sqrt(theta0[c("sig2M","sig2F")])      # store sigmas
alpha = theta0["alpha"]                       # store alpha
pMh   = dnorm(his,mu0s[1],sig0s[1])*alpha     # prob data and male
pFh   = dnorm(his,mu0s[2],sig0s[2])*(1-alpha) # prob data and female
piMh  = pMh/(pMh+pFh)                         # prob male given data

#M-Step
theta1["muM"]   = mean_wgt(his,piMh)     # male mean
theta1["muF"]   = mean_wgt(his,1-piMh)   # female mean
theta1["sig2M"] = var_wgt(his,piMh)      # male variance
theta1["sig2F"] = var_wgt(his,1-piMh)    # female variance
theta1["alpha"] = prop_wgt(piMh)         # prob male

cbind(theta0,theta1)
```

```
##           theta0     theta1
## muM   70.6040213 70.4555324
## muF   64.0273771 64.1556827
## sig2M  4.4340449  5.3987530
## sig2F  4.7091539  5.5277721
## alpha  0.4177237  0.4157106
```

---
# EM Algorithm: While Loop


``` r
## ---- EM Loop: Run Until Convergence
while (norm(theta0 - theta1,p=Inf) &gt; 1e-12) {
  ## ---- E-Step
  theta0 = theta1                                # update theta0
  mu0s   = theta0[c("muM","muF")]                # store mus
  sig0s  = sqrt(theta0[c("sig2M","sig2F")])      # store sigmas
  alpha  = theta0["alpha"]                       # store alpha
  pMh    = dnorm(his,mu0s[1],sig0s[1])*alpha     # prob data and male
  pFh    = dnorm(his,mu0s[2],sig0s[2])*(1-alpha) # prob data and female
  piMh   = pMh/(pMh+pFh)                         # prob male given data

  #M-Step
  theta1["muM"]   = mean_wgt(his,piMh)     # male mean
  theta1["muF"]   = mean_wgt(his,1-piMh)   # female mean
  theta1["sig2M"] = var_wgt(his,piMh)      # male variance
  theta1["sig2F"] = var_wgt(his,1-piMh)    # female variance
  theta1["alpha"] = prop_wgt(piMh)         # prob male
}
theta1
```

```
##        muM        muF      sig2M      sig2F      alpha 
## 69.5209013 63.7703840  8.2256854  5.3738272  0.5224249
```

---
# EM Algorithm: Results


``` r
EM_moms   = data.table(sex  = c("Male","Female"),
                       mu   = theta1[c("muM","muF")],
                       sig2 = theta1[c("sig2M","sig2F")],
                       prob = c(theta1["alpha"],1-theta1["alpha"]))
hght_moms = cbind(hght_moms,data.table(prob=c(mean(NHIS_data$sex=="Male"),
                                              mean(NHIS_data$sex=="Female"),1)))
```

Mean and variance by sex estimated with EM algorithm

```
##       sex       mu     sig2      prob
##    &lt;char&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;
## 1:   Male 69.52090 8.225685 0.5224249
## 2: Female 63.77038 5.373827 0.4775751
```

Mean and variance by sex grouping by the sex variable in the data

```
##       sex       mu      var      prob
##    &lt;char&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;
## 1:   Male 69.79632 7.953308 0.4677408
## 2: Female 64.11915 6.332597 0.5322592
```

--

They are pretty similar, though the baseline probabilities `\(\alpha\)` are "switched."
- This is a known problem with the EM algorithm.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
