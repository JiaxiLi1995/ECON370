<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Drew Van Kuiken" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Data Science for Economists
]
.subtitle[
## Lecture 16: Intro to Data Science
]
.author[
### Drew Van Kuiken
]
.date[
### University of North Carolina | <a href="https://github.com/drewvankuiken/ECON370">ECON 370</a>
]

---








# Table of contents

1. [Introduction](#intro)

2. [What is data science?](#datascience)

3. [What do data scientists do?](#whatdo)

4. [Modeling](#modeling)

5. [What do economists bring?](#economists)

---
class: inverse, center, middle
name: intro

# Introduction

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Motivation

We've spent most the class learning R and becoming competent in programming. 

Today, we'll do a quick scan of the data science world and the things that R can be used for in the workplace.

Remember that I do not expect you to entirely understand everything we talk about.
- I want you to get a taste so that when you really learn this stuff later, it is easier.

---
class: inverse, center, middle
name: datascience

# What Is Data Science?

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# What is data science?

Data science is a relatively new "field" that is still evolving.

--

Wikipedia's definition:

&gt; Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.

--

Another definition:

&gt; Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician. - [Josh Wills](https://twitter.com/josh_wills)

--

Anyone can be a data scientist!
- Economists have a special toolkit that is more important for data science than ever.

---
class: inverse, center, middle
name: whatdo

# What do data scientists do?

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# What do data scientists do?

1. Data mining: extracting, wrangling, and storing large amounts of data. 

2. Modeling: applying models and ideas from both statistical/machine learning and traditonal statistics to build algorithms to do things too difficult for humans.

3. Software/website development: some data scientists will take the data, algorithms, and insights they develop and integrated them into software or websites.

--

There are lots of buzzwords in this area. It is important to see through this and not get intimidated. 

--

We've already spent a bit of time talking about data wrangling.

Let's talking about the modeling side.

---
# What do data scientist do?

![](pics/DataScientistDo.jpg)
---
class: inverse, center, middle
name: model

# Modeling
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---
# Statistics vs Machine Learning

One of the most confusing thing more many people is understanding the difference between machine learning and traditional statistics.

In truth, there isn't much of a difference and there's *lots* of overlap between the two.
- E.g. Ridge and LASSO regression are used in both and many statistical learning algorithms are just "flexible estimation" (nonparametric estimation) in statistics.

The big difference comes down to philosophical differences and objects of interest.

Suppose you have a traditional linear model: `$$y_i = x_i \beta + \varepsilon_i$$`

- Statisticians will care about getting an estimate for `\(\beta\)`, called `\(\hat{\beta}\)`
 - Ideally, `\(\hat{\beta}\)` will have desirable properties.
- Machine learning cares about getting accurate and "precise" estimates of `\(y_i\)`, called `\(\hat{y}_i\)`

This difference comes down to inference of `\(\hat{\beta}\)` versus prediction of `\(\hat{y}_i\)`.

---
# Inference vs Prediction

Since we do not know the true value of `\(\beta\)` we want to estimate it using an "estimator."
- An estimator is a function `\(\hat{\beta}\)` that takes data `\(\boldsymbol{Z}\)` (for our example, `\(\boldsymbol{Z}=(y_i,x_i)_{i=1}^{N}\)`) as an input and returns an estimate of `\(\beta\)`.
- For a given sample `\(\boldsymbol{Z}\)`,  `\(\hat{\beta}(\boldsymbol{Z})\)` is an estimate for `\(\beta\)`. Will just write `\(\hat{\beta}\)` from now on, but remember that estimators are functions of your *data*.

One approach to estimation is to choose an estimator that minimizes the "mean-squared error" (MSE)
- The MSE is the average squared difference between the estimate `\(\hat{\beta}\)` and the true value `\(\beta\)` `$$\text{MSE}_\beta(\hat{\beta}) = E[(\hat{\beta}- \beta)^2]$$`

However, different estimators can minimize the MSE but have different properties. Using algebra, we can write the MSE like so:
`$$\text{MSE}_\beta(\hat{\beta}) = Bias(\hat{\beta},\beta)^2  + Var(\hat{\beta})$$`
This decomposition of the MSE encapsulates the different goals of traditional statistics (inference) and machine learning (prediction).

---
# Bias-Variance Trade-Off

Suppose we have two estimators `\(\hat{\beta}\)` and `\(\tilde{\beta}\)` that result in *the same* `\(\text{MSE}_\beta=\bar{m}\)`  
- For estimator `\(x\)`, let `\(\varepsilon(x)=Bias(x,\beta)=E[x]-\beta\)` and `\(\Sigma(x)=Var(x)\)`.

--

Since `\(\varepsilon(\hat{\beta})^2+\Sigma(\hat{\beta})=\bar{m}\)` and `\(\varepsilon(\tilde{\beta})^2+\Sigma(\tilde{\beta})=\bar{m}\)`, `\(\varepsilon(\hat{\beta})^2+\Sigma(\hat{\beta})=\varepsilon(\tilde{\beta})^2+\Sigma(\tilde{\beta})\)` or `$$\varepsilon(\hat{\beta})^2-\varepsilon(\tilde{\beta})^2 = \Sigma(\tilde{\beta})-\Sigma(\hat{\beta}) \iff (\varepsilon(\hat{\beta})-\varepsilon(\tilde{\beta}))(\varepsilon(\hat{\beta})+\varepsilon(\tilde{\beta})) = \Sigma(\tilde{\beta})-\Sigma(\hat{\beta})$$`

1. If `\(\Sigma(\tilde{\beta})=\Sigma(\hat{\beta})\)`, it must be `\(|\varepsilon(\hat{\beta})|=|\varepsilon(\tilde{\beta})|\)`.
2. If `\(\Sigma(\tilde{\beta})&gt;\Sigma(\hat{\beta})\)`, it must be `\(|\varepsilon(\hat{\beta})|&gt;|\varepsilon(\hat{\beta})|\)`.

--

**Conclusion**: If one estimator has a lower variance than another and they both have the same MSE, it must be the one with lower variance has a larger bias.

--

1. For a fixed level of MSE `\(m(x)=\bar{m}\)`, `\(\frac{d \text{ }\Sigma(x)}{d \text{ }\varepsilon(x)}=-2\varepsilon(x)\)`.
2. `\(\frac{d m(x)}{d \varepsilon(x)}=2 \varepsilon(x)\)` and `\(\frac{d m(x)}{d \Sigma(x)}=1\)`, so `\(|\frac{d m(x)}{d \varepsilon(x)}|&lt;|\frac{d m(x)}{d \Sigma(x)}| \iff |\varepsilon(x)|&lt;\frac{1}{2}\)`

--

**Conclusion**: An increase in the bias is substituted with a larger decrease in the variance. As well, accepting some bias increases MSE less than increasing the variance as long as the bias is "small."

---
# Inference vs Prediction (cont.)

Traditional statistics `\(\rightarrow\)` inference of `\(\beta\)`
- We want to try to infer the value of `\(\beta\)` from the data.
- Choose an estimator `\(\hat{\beta}\)` that has low variance and low bias to perform "powerful" statistical tests and make conclusions about the value of `\(\beta\)`.
- Usually (though not always) this requires us to choose an unbiased estimator so we aren't "skewing" the estimates `\(\hat{\beta}\)` too much.

--

Machine Learning `\(\rightarrow\)` predicting `\(y\)`
- Estimate some model given "training data" and then predict the value of `\(y\)` given a new value of `\(x\)`. 
- Call this prediction `\(\hat{y}\)`. In our example, `\(\hat{y}=x\hat{\beta}\)`
- Usually want the variance of the prediction to be small (noisy predictions are useless).
- In our example, given a new `\(x\)`, we have `\(Var(\hat{y})=Var(x\hat{\beta})=x^2Var(\hat{\beta})\)`.
- To decrease the variance of `\(\hat{y}\)`, we must decrease the variance of `\(\hat{\beta}\)`.
- Leads to choosing estimators with some bias if it decreases the variance enough.

---
# Inference vs Prediction (cont.)

![](pics/ridge_meme.jpeg)

---

# Inference vs Prediction (cont.)

Prediction Problems:
1. How much will this ad campaign increase revenue?
2. What will traffic on the website be tomorrow?
3. Is this tweet harmful and should it be flagged?
4. What will the price of a home be in a year?

Inference Problems:
1. Will this ad campaign have a causal impact on revenue?
2. Does education have a causal effect on wages?
3. Will changing the Twitter UI cause people to spend more time on the site?
4. What is the causal effect of renovating a kitchen on a home price?

--

For more on the difference, check out [this blog post](https://ryxcommar.com/2019/07/14/on-moving-from-statistics-to-machine-learning-the-final-stage-of-grief/) by r y x, r 

---
# Types of Learning

- Supervised Learning:
 - You have a target variable ("dependent variable") and you would like to learn from function of the features ("independent variables") that explains the target.

--
- Unsupervised Learning:
 - We observe `\(X\)`, but not `\(y\)`. While we can't use traditional statistical models, we can still do things like "clustering," classify observations of `\(X\)` based on similarity.

--
 
There are two main types of supervised learning:

1\. Regression: the target variable, `\(y\)`, is continuous and you want to learn a function `\(f\)` of the features `\(X\)` where `\(y = f(X)+\varepsilon\)` where `\(\varepsilon\)` is some error term.
- If `\(\hat{f}\)` is your estimate of `\(f\)` (or "learned function"), then the prediction of `\(y\)`, called `\(\hat{y}\)` is `\(\hat{y} = \hat{f}(X)\)`

--

2\. Classification: the target variable `\(y\)` is a category (e.g. `\(y =\)` freshman, sophomore, junior, senior) and you want to learn `\(P(Y=y | X)\)` so if you're given a new observation of `\(X\)`, you can predict which group it belongs to.

---
# Types of Learning (cont.)

Below are some examples of each type you may have heard of.

- Regression\*: linear regression, Ridge regression, LASSO regression.
- Classification: logistic regression, K-nearest neighbors.
- Unsupervised learning: K-means.

--

\* Don't confuse regression in the learning sense with regression in the statistical sense. While they are similar and have the same name, they are different. When we say linear regression, we are referring to estimating a condition mean `\(E[y|X]\)` with a linear model. Regression in machine learning is any model where `\(y\)` is continuous regardless of what's being estimated.

---
class: inverse, center, middle
name: economists

# What do economists have to bring?
&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;


---
# What do economists have to bring?

As economists, we bring a lot of unique tools to the world of data science.

--

Our specialty is being able to think carefully about observational data to obtain causal effects or "counterfactuals."

--

This is where the idea of the "data generating process" comes in handy. How did the data come to us? What economic choices affect how we observe the data? 

--

The Nobel (Memorial) Prize in Economics was award to Angrist, Imbens, and Card in part for their work on developing ideas an econometric framework to think about causal questions seriously.

--

In fact, Twitter Engineering ([@TwitterEng](https://twitter.com/TwitterEng)) created a [tweet thread about how Angrist's and Imbens's work have influenced their work at Twitter](https://twitter.com/TwitterEng/status/1450179475426066433?s=20) that is worth a read.

--

Econometricians have contributed to the field of machine learning; see [Athey et. al (2019)](https://arxiv.org/abs/1610.01271), [Athey and Imbens (2016)](https://www.pnas.org/content/113/27/7353), [Nekipelov et. al. (2021)](https://cpb-us-w2.wpmucdn.com/sites.wustl.edu/dist/5/501/files/2016/10/momentTrees.pdf), just to name a few.

---
# What do economists have to bring?


Many of the issues we face in economics are the same issues data scientists are finding cause problems with their models. 
- If you train an algorithm for resumes on only white men, what do you think the algorithm will do when it gets a resume from someone not white or male?

- The types of people who select into using Twitter are likely different than those who don't use Twitter. How/when should Twitter keep this in mind when training their algorithms?

- How does racial bias in incarceration rates affect algorithms used to recommend probation or sentencing? (Yes, these exist.)

- How does endogeneity in credit/financial history affect the credit scores assigned to individuals?


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
